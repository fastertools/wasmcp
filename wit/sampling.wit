package fastertools:mcp@0.1.1;

/// LLM sampling interface - allows servers to request AI assistance from clients
/// This enables powerful agent workflows where servers can leverage client LLM access

interface sampling {
    use types.{
        mcp-error,
        content-block,
        message-role,
        model-preferences,
        meta-fields
    };

    /// Message in a conversation with the LLM
    record sampling-message {
        /// Role of the message sender
        role: message-role,
        /// Content of the message
        content: content-block,
    }

    /// Request to create/sample a message from an LLM
    record create-message-request {
        /// Conversation messages to send to the LLM
        messages: list<sampling-message>,
        /// Optional model selection preferences
        model-preferences: option<model-preferences>,
        /// System prompt to guide the LLM
        system-prompt: option<string>,
        /// Request to include MCP context from servers
        include-context: option<string>,
        /// Sampling temperature (0.0-2.0, higher = more creative)
        temperature: option<f64>,
        /// Maximum tokens to generate
        max-tokens: s32,
        /// Sequences that stop generation
        stop-sequences: option<list<string>>,
        /// Provider-specific metadata
        metadata: option<meta-fields>,
    }

    /// Response from LLM sampling
    record create-message-result {
        /// Role of the generated message (usually assistant)
        role: message-role,
        /// Generated content
        content: content-block,
        /// Model that was used
        model: string,
        /// Reason generation stopped (e.g., "stop", "max_tokens")
        stop-reason: option<string>,
        /// Extension metadata
        meta: option<meta-fields>,
    }

    /// Request LLM sampling from the client
    /// The client has full discretion over model selection and may
    /// inform the user before sampling (human in the loop)
    create-message: func(request: create-message-request) -> result<create-message-result, mcp-error>;
}