/// LLM sampling types for MCP (CLIENT CAPABILITY)
/// 
/// NOTE: This is a CLIENT capability - servers request sampling, clients provide it.
/// Only type definitions are provided here, no function interfaces, as:
/// 1. No real MCP client implementations exist yet
/// 2. Client capability interfaces are likely to change in future MCP spec versions
/// 3. The exact client-side implementation patterns are still being established
///
/// These types enable protocol-compliant marshaling while avoiding premature
/// interface definitions for capabilities that don't have reference implementations.

/// Type definitions for sampling
interface sampling-types {
    use types.{
        content-block,
        message-role,
        model-preferences,
        meta-fields
    };

    /// Message in a conversation with the LLM
    record sampling-message {
        /// Role of the message sender
        role: message-role,
        /// Content of the message
        content: content-block,
    }

    /// Request to create/sample a message from an LLM
    record create-message-request {
        /// Conversation messages to send to the LLM
        messages: list<sampling-message>,
        /// Optional model selection preferences
        model-preferences: option<model-preferences>,
        /// System prompt to guide the LLM
        system-prompt: option<string>,
        /// Request to include MCP context from servers
        include-context: option<string>,
        /// Sampling temperature (0.0-2.0, higher = more creative)
        temperature: option<f64>,
        /// Maximum tokens to generate
        max-tokens: s32,
        /// Sequences that stop generation
        stop-sequences: option<list<string>>,
        /// Provider-specific metadata
        metadata: option<meta-fields>,
    }

    /// Response from LLM sampling
    record create-message-result {
        /// Role of the generated message (usually assistant)
        role: message-role,
        /// Generated content
        content: content-block,
        /// Model that was used
        model: string,
        /// Reason generation stopped (e.g., "stop", "max_tokens")
        stop-reason: option<string>,
        /// Extension metadata
        meta: option<meta-fields>,
    }
}